# tournament-sim

## Purpose

The purpose of this software library is to simulate virtual FIRST Tech Challenge tournaments. Certain parameters of a tournament can be specified:
- Number of teams
- Number of matches per team
- Tiebreaker point (TBP) method

A simulator is useful for obtaining results that would be difficult or impossible to obtain in real life. We used this simulator to run 10,000 synthetic tournaments for a variety of tournament configurations and tiebreaker point methods. Thus, each data point is based on more tournaments than have happened in the history of FTC. Due to the law of large numbers, we have confidence in the accuracy of the data.

## Results

For several tournament sizes, we tested four TBP methods: losing alliance score (the current method), sum of both alliances' scores, your alliance's score, and your team's OPR. For each TBP method, we calculated the average root-mean-square deviation between the input rankings and the output rankings of all the teams for 10,000 tournaments. The RMSD represents the input/output difference in ranking for an average team in a given tournament. In the table below, the leftmost two columns indicate the tournament type, while the rightmost four columns indicate the corresponding average RMSD for each TBP method.

Teams |Matches per Team|Current| Sum  |Yours | OPR
------|----------------|-------|------|------|------
16    |5               |2.972  |2.876 |2.781 |2.489
24    |5               |4.600  |4.477 |4.360 |4.010
32    |5               |6.195  |6.063 |5.901 |5.437
32    |6               |5.798  |5.682 |5.549 |5.177
40    |8               |6.586  |6.461 |6.340 |6.003
40    |9               |6.285  |6.165 |6.039 |5.761
80    |9               |13.043 |12.812|12.630|12.111

![A nice graph of the data from the above table](https://github.com/ftc9899/tournament-sim/blob/master/tbp_comparison.PNG?raw=true)

![Another graph](https://github.com/ftc9899/tournament-sim/blob/master/tbp_percent_comparison.PNG?raw=true)

## Conclusions

Here is what can be concluded from this data:
- In our simulations, tiebreaker points contributed up to 24% of the difference between a team's input and output ranking
  - Using the TBP method of OPR, any differences in a team's input and output rank are due to RP alone
  - The contribution of RP to the RMSD is shown by the OPR TBP method
  - The contribution of a TBP method to the RMSD is the change in RMSD from the OPR method to the TBP method
- The chosen TBP method has a greater effect on the output ranking of high-scoring teams as compared to all teams
  - In the case of a Worlds-level tournament, the RP contribute an average difference of 6.162 ranking positions
  - The current TBP method contributes an additional average difference of 1.424 ranking positions, which is 18.77% of the total difference
  - The 'yours' TBP method contributes an additional average difference of 0.407 ranking positions, which is 6.20% of the total difference
- Using an accurate OPR as the TBP method results in the least difference between input and output rankings
  - However, we are not recommending OPR as a TBP method because the accuracy and convenience of the simulated OPR is not attainable in real life
- Here are the other TBP methods in order of increasing difference between input and output rankings:
  - Your alliance's score
  - Sum of both alliances' scores
  - Losing alliance's score
- A tournament with 40 teams each playing 9 matches has an average ranking difference less than half that of a tournament with 80 teams each playing 9 matches
  - Therefore, it may be useful to consider organizing the FTC World Championships as 4 divisions of 40 teams each, though this would add an extra round of playoffs
  - If there is not enough time for such a tournament, note that a division of 40 teams each playing 8 matches has only a slightly larger average ranking difference

## Algorithm Overview

### Generate Teams

This algorithm generates the specified number of teams, each with a scoring potential following a normal distribution generated by NumPy. The distribution has an average, standard deviation, and range that follow the 2019 Houston World Championship OPR distribution, which approximated a normal distribution. 

### Create Match Schedule

The scheduling algorithm follows the [official algorithm](https://idleloop.com/matchmaker/) when it matters. This is the element of the official algorithm that this algorithm incorporates:
- Pairing Uniformity
  - A given team will not partner with any other team more than once
  - A given team will not face a given opponent more than once

These are the elements of the official algorithm that are not necessary for a simulation:
- Round Uniformity
- Match Separation
- Red/Blue Balancing
- Station Position Balancing

This algorithm only handles tournaments where number of teams is a multiple of 4, so Surrogate Appearances are not needed.

### Simulate the Qualification Matches

After a match schedule has been created, an alliance's score in a given match is calculated by adding the scoring potential of both teams. To simulate the real-life variance of a team's score from match to match, each team's score in a given match is varied by Â±10%. When the winner and loser of a match have been determined, RP and TBP are assigned accordingly. For the 'OPR' TBP method, the team's scoring potential (without variance) is added after each match.

## Using this library

`main.py` is set up to run any number of tournaments with a specified number of teams and matches per team. The user can select one of four built-in TBP methods. `main.py` is designed to be run from a terminal with the following options:
- `-t` or `--teams` followed by the number of teams
- `-m` or `--matches` followed by the number of matches per team
- `-n` or `--tournaments` followed by the number of tournaments to run
- `-r` or `--ranking` followed by the TBP method to use ('current', 'sum', 'yours', or 'opr')